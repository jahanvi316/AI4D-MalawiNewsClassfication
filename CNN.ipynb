{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading necessory packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/trdapps/linux-x86_64/envs/BKV_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/trdapps/linux-x86_64/envs/BKV_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/trdapps/linux-x86_64/envs/BKV_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/trdapps/linux-x86_64/envs/BKV_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/trdapps/linux-x86_64/envs/BKV_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/trdapps/linux-x86_64/envs/BKV_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/trdapps/linux-x86_64/envs/BKV_env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/trdapps/linux-x86_64/envs/BKV_env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/trdapps/linux-x86_64/envs/BKV_env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/trdapps/linux-x86_64/envs/BKV_env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/trdapps/linux-x86_64/envs/BKV_env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/trdapps/linux-x86_64/envs/BKV_env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "import pandas, numpy, string\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Train dataset ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset before preprocessing:  1436\n",
      "\n",
      " We also have 20 different labels/class including ['POLITICS' 'HEALTH' 'LAW/ORDER' 'RELIGION' 'FARMING'\n",
      " 'WILDLIFE/ENVIRONMENT' 'SOCIAL ISSUES' 'SOCIAL' 'OPINION/ESSAY'\n",
      " 'LOCALCHIEFS' 'WITCHCRAFT' 'ECONOMY' 'SPORTS' 'RELATIONSHIPS' 'TRANSPORT'\n",
      " 'CULTURE' 'EDUCATION' 'MUSIC' 'ARTS AND CRAFTS' 'FLOODING'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('Train.csv')\n",
    "print('Length of dataset before preprocessing: ', len(df))\n",
    "print('\\n We also have {} different labels/class including {} \\n'.format(len(df['Label'].unique()), df['Label'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each sample in raw dataset include a col that contains 15-16 different sentences...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ID_AASHwXxg</td>\n",
       "      <td>Mwangonde: Khansala wachinyamata Akamati achi...</td>\n",
       "      <td>POLITICS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ID_AGoFySzn</td>\n",
       "      <td>MCP siidakhutire ndi kalembera Chipani cha Ma...</td>\n",
       "      <td>POLITICS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>ID_AGrrkBGP</td>\n",
       "      <td>Bungwe la MANEPO Lapempha Boma Liganizire Anth...</td>\n",
       "      <td>HEALTH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ID_AIJeigeG</td>\n",
       "      <td>Ndale zogawanitsa miyambo zanyanya Si zachile...</td>\n",
       "      <td>POLITICS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ID_APMprMbV</td>\n",
       "      <td>Nanga wapolisi ataphofomoka? Masiku ano sichi...</td>\n",
       "      <td>LAW/ORDER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1431</td>\n",
       "      <td>ID_zmTmmEio</td>\n",
       "      <td>Eni Minibus Ati Ali ndi Ufulu Wokweza Mitengo ...</td>\n",
       "      <td>TRANSPORT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1432</td>\n",
       "      <td>ID_znOlIaGQ</td>\n",
       "      <td>Kachali apepesa: Kulankhula motumbwa kuthe An...</td>\n",
       "      <td>POLITICS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1433</td>\n",
       "      <td>ID_znracTjN</td>\n",
       "      <td>Mawu supports non-fiction writers The Malawi ...</td>\n",
       "      <td>EDUCATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1434</td>\n",
       "      <td>ID_ztdsmmva</td>\n",
       "      <td>Tame Mwawa: Phwete ndiye kudya kwake Sewero l...</td>\n",
       "      <td>SOCIAL ISSUES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1435</td>\n",
       "      <td>ID_zteydTpN</td>\n",
       "      <td>PAC iunguza za boma la chifedulo Nthumwi zomw...</td>\n",
       "      <td>POLITICS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1436 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               ID                                               Text  \\\n",
       "0     ID_AASHwXxg   Mwangonde: Khansala wachinyamata Akamati achi...   \n",
       "1     ID_AGoFySzn   MCP siidakhutire ndi kalembera Chipani cha Ma...   \n",
       "2     ID_AGrrkBGP  Bungwe la MANEPO Lapempha Boma Liganizire Anth...   \n",
       "3     ID_AIJeigeG   Ndale zogawanitsa miyambo zanyanya Si zachile...   \n",
       "4     ID_APMprMbV   Nanga wapolisi ataphofomoka? Masiku ano sichi...   \n",
       "...           ...                                                ...   \n",
       "1431  ID_zmTmmEio  Eni Minibus Ati Ali ndi Ufulu Wokweza Mitengo ...   \n",
       "1432  ID_znOlIaGQ   Kachali apepesa: Kulankhula motumbwa kuthe An...   \n",
       "1433  ID_znracTjN   Mawu supports non-fiction writers The Malawi ...   \n",
       "1434  ID_ztdsmmva   Tame Mwawa: Phwete ndiye kudya kwake Sewero l...   \n",
       "1435  ID_zteydTpN   PAC iunguza za boma la chifedulo Nthumwi zomw...   \n",
       "\n",
       "              Label  \n",
       "0          POLITICS  \n",
       "1          POLITICS  \n",
       "2            HEALTH  \n",
       "3          POLITICS  \n",
       "4         LAW/ORDER  \n",
       "...             ...  \n",
       "1431      TRANSPORT  \n",
       "1432       POLITICS  \n",
       "1433      EDUCATION  \n",
       "1434  SOCIAL ISSUES  \n",
       "1435       POLITICS  \n",
       "\n",
       "[1436 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Each sample in raw dataset include a col that contains 15-16 different sentences...\\n')\n",
    "df.head(1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating new table including refID of news agency, sentence in the news, Label of each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_data = pd.DataFrame(columns=['refID', 'Sentence', 'Label'])\n",
    "for idx in range(len(df)):\n",
    "    tmp = df.loc[idx]['Text'].split('\\n')\n",
    "    for jdx in range(len(tmp)):\n",
    "        X_data = X_data.append({'refID': df.loc[idx]['ID'],'Sentence': tmp[jdx], 'Label': df.loc[idx]['Label']}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocessing all samples by changing them to lowercase and erasing punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying preprocessing steps : Changing all characters to lower-case and removing punctuation\n"
     ]
    }
   ],
   "source": [
    "print('Applying preprocessing steps : Changing all characters to lower-case and removing punctuation')\n",
    "X_data.loc[:,\"Sentence\"] = X_data.Sentence.apply(lambda x : str.lower(x))\n",
    "X_data.loc[:,\"Label\"] = X_data.Label.apply(lambda x : str.lower(x))\n",
    "X_data.loc[:,\"Sentence\"] = X_data.Sentence.str.replace('[^\\w\\s]','')\n",
    "X_data.loc[:,\"Sentence\"] = X_data.Sentence.apply(lambda x : x.translate(string.punctuation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "changing Labels to indecies ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encode the target variable \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "X_data.loc[:,\"Label\"] = encoder.fit_transform(X_data.Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>refID</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ID_AASHwXxg</td>\n",
       "      <td>mwangonde khansala wachinyamata akamati achin...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ID_AASHwXxg</td>\n",
       "      <td>mbiri ya maphunziro anu ndi yotani maphunziro...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>ID_AASHwXxg</td>\n",
       "      <td>mudayamba bwanji zandale kuyambira ndili wach...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ID_AASHwXxg</td>\n",
       "      <td>ntchito mukugwira ndi zomwe munkayembekezera ...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ID_AASHwXxg</td>\n",
       "      <td>masomphenya anu ndi otani pandale ine ndine m...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16496</td>\n",
       "      <td>ID_zteydTpN</td>\n",
       "      <td>phiri adati zomwe adakambirana akuluakuluwo a...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16497</td>\n",
       "      <td>ID_zteydTpN</td>\n",
       "      <td>mmbuyomu kafukufuku yemwe nyuzipepala ya the ...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16498</td>\n",
       "      <td>ID_zteydTpN</td>\n",
       "      <td>mchaka cha 2006 mtsogoleri wa dziko lino pete...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16499</td>\n",
       "      <td>ID_zteydTpN</td>\n",
       "      <td>pamsonkhanowo padali a zipani zosiyanasiyana ...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>ID_zteydTpN</td>\n",
       "      <td></td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16501 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             refID                                           Sentence  Label\n",
       "0      ID_AASHwXxg   mwangonde khansala wachinyamata akamati achin...     11\n",
       "1      ID_AASHwXxg   mbiri ya maphunziro anu ndi yotani maphunziro...     11\n",
       "2      ID_AASHwXxg   mudayamba bwanji zandale kuyambira ndili wach...     11\n",
       "3      ID_AASHwXxg   ntchito mukugwira ndi zomwe munkayembekezera ...     11\n",
       "4      ID_AASHwXxg   masomphenya anu ndi otani pandale ine ndine m...     11\n",
       "...            ...                                                ...    ...\n",
       "16496  ID_zteydTpN   phiri adati zomwe adakambirana akuluakuluwo a...     11\n",
       "16497  ID_zteydTpN   mmbuyomu kafukufuku yemwe nyuzipepala ya the ...     11\n",
       "16498  ID_zteydTpN   mchaka cha 2006 mtsogoleri wa dziko lino pete...     11\n",
       "16499  ID_zteydTpN   pamsonkhanowo padali a zipani zosiyanasiyana ...     11\n",
       "16500  ID_zteydTpN                                                        11\n",
       "\n",
       "[16501 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split the dataset into training and validation datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(X_data['Sentence'], X_data['Label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "approach 1 : encoding words using raw term count features for LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(X_data['Sentence'])\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "xvalid_count =  count_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "approach 2 : encoding words using rExtracting tf-idf features for NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram.fit(X_data['Sentence'])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    return metrics.accuracy_score(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, Count Vectors:  0.4650993698497334\n",
      "NB, N-Gram Vectors:  0.38342220067862337\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes on Count Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
    "print('NB, Count Vectors: ', accuracy)\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print('NB, N-Gram Vectors: ', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, Count Vectors:  0.2433349491032477\n",
      "SVM, N-Gram Vectors:  0.2433349491032477\n"
     ]
    }
   ],
   "source": [
    "accuracy = train_model(svm.SVC(), xtrain_count, train_y, xvalid_count)\n",
    "print('SVM, Count Vectors: ', accuracy)\n",
    "\n",
    "accuracy = train_model(svm.SVC(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print('SVM, N-Gram Vectors: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, Count Vectors:  0.422200678623364\n",
      "RF, N-Gram Vectors:  0.34827920504120213\n"
     ]
    }
   ],
   "source": [
    "# RF on Count Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_count, train_y, xvalid_count)\n",
    "print('RF, Count Vectors: ', accuracy)\n",
    "\n",
    "# RF on Word Level TF IDF Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print('RF, N-Gram Vectors: ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO apply deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, GlobalMaxPooling1D, Conv1D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14922, (12375,), (4126,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_data['Sentence'].unique()), train_x.shape, valid_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "all_words = []\n",
    "for sent in X_data['Sentence']:\n",
    "    tokenize_word = word_tokenize(sent)\n",
    "    for word in tokenize_word:\n",
    "        all_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51137\n"
     ]
    }
   ],
   "source": [
    "unique_words = set(all_words)\n",
    "print(len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_sentences = [one_hot(sent, 50) for sent in X_data['Sentence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = lambda sentence: len(word_tokenize(sentence))\n",
    "longest_sentence = max(X_data['Sentence'], key=word_count)\n",
    "length_long_sentence = len(word_tokenize(longest_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33  2 41 13  7 11 23  7 48 49 47 47 13 11 33 45  6  7  4 25 37 33 42 10\n",
      " 23 46 35 47 27  6 13 41  8 49 12 20 20 20  7 11 33  2 13 36 40 24 20 37\n",
      " 43  4 18  4 20 34  9 27 12 49 23 20 35 32 27  8 11  1 33 49 40 37 33 21\n",
      " 11 11 24 28  6 21 40 33  9  1 40  5  8 37  2 37 45 31 47 20 36 22 40 23\n",
      " 20 38 35  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "padded_sentences = pad_sequences(embedded_sentences, length_long_sentence, padding='post')\n",
    "print(padded_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /trdapps/linux-x86_64/envs/BKV_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /trdapps/linux-x86_64/envs/BKV_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /trdapps/linux-x86_64/envs/BKV_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /trdapps/linux-x86_64/envs/BKV_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /trdapps/linux-x86_64/envs/BKV_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /trdapps/linux-x86_64/envs/BKV_env/lib/python3.6/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /trdapps/linux-x86_64/envs/BKV_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 472, 20)           1000      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 472, 20)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 468, 256)          25856     \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 464, 128)          163968    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                1300      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 20)                0         \n",
      "=================================================================\n",
      "Total params: 200,380\n",
      "Trainable params: 200,380\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_dims, filters, kernel_size, hidden_dims = 50, 256, 5, 64\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(50, 20, input_length=length_long_sentence))\n",
    "model.add(Dropout(0.2)) # masks various input values\n",
    "    \n",
    "# Create the convolutional layer\n",
    "model.add(Conv1D(filters, kernel_size,padding='valid', activation='relu', strides=1))\n",
    "\n",
    "# Create the convolutional layer\n",
    "model.add(Conv1D(128, kernel_size,padding='valid', activation='relu', strides=1))\n",
    "\n",
    "# Create the pooling layer\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# Create the fully connected layer\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Create the output layer (num_classes)\n",
    "model.add(Dense(20))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# Add optimization method, loss function and optimization value\n",
    "opt = keras.optimizers.Adam(lr=0.001)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=opt, metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "embeding_labels = to_categorical(X_data['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeding_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = model_selection.train_test_split(padded_sentences, embeding_labels, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /trdapps/linux-x86_64/envs/BKV_env/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 12375 samples, validate on 4126 samples\n",
      "Epoch 1/50\n",
      "12375/12375 [==============================] - 28s 2ms/step - loss: 2.6037 - acc: 0.2311 - val_loss: 2.5632 - val_acc: 0.2365\n",
      "Epoch 2/50\n",
      "12375/12375 [==============================] - 28s 2ms/step - loss: 2.5607 - acc: 0.2373 - val_loss: 2.5282 - val_acc: 0.2365\n",
      "Epoch 3/50\n",
      "12375/12375 [==============================] - 28s 2ms/step - loss: 2.5143 - acc: 0.2444 - val_loss: 2.4903 - val_acc: 0.2475\n",
      "Epoch 4/50\n",
      "12375/12375 [==============================] - 28s 2ms/step - loss: 2.4801 - acc: 0.2477 - val_loss: 2.4742 - val_acc: 0.2511\n",
      "Epoch 5/50\n",
      "12375/12375 [==============================] - 28s 2ms/step - loss: 2.4522 - acc: 0.2553 - val_loss: 2.4722 - val_acc: 0.2494\n",
      "Epoch 6/50\n",
      "12375/12375 [==============================] - 28s 2ms/step - loss: 2.4321 - acc: 0.2590 - val_loss: 2.4561 - val_acc: 0.2516\n",
      "Epoch 7/50\n",
      "12375/12375 [==============================] - 28s 2ms/step - loss: 2.4034 - acc: 0.2658 - val_loss: 2.4661 - val_acc: 0.2525\n",
      "Epoch 8/50\n",
      "12375/12375 [==============================] - 28s 2ms/step - loss: 2.3714 - acc: 0.2709 - val_loss: 2.4443 - val_acc: 0.2477\n",
      "Epoch 9/50\n",
      "12375/12375 [==============================] - 28s 2ms/step - loss: 2.3473 - acc: 0.2735 - val_loss: 2.4598 - val_acc: 0.2458\n",
      "Epoch 10/50\n",
      "12375/12375 [==============================] - 28s 2ms/step - loss: 2.3176 - acc: 0.2835 - val_loss: 2.4514 - val_acc: 0.2499\n",
      "Epoch 11/50\n",
      "12375/12375 [==============================] - 28s 2ms/step - loss: 2.2876 - acc: 0.2890 - val_loss: 2.4676 - val_acc: 0.2492\n",
      "Epoch 12/50\n",
      "12375/12375 [==============================] - 28s 2ms/step - loss: 2.2494 - acc: 0.3021 - val_loss: 2.4686 - val_acc: 0.2540\n",
      "Epoch 13/50\n",
      "12375/12375 [==============================] - 28s 2ms/step - loss: 2.2217 - acc: 0.3071 - val_loss: 2.4991 - val_acc: 0.2455\n",
      "Epoch 14/50\n",
      "12375/12375 [==============================] - 28s 2ms/step - loss: 2.1950 - acc: 0.3109 - val_loss: 2.4934 - val_acc: 0.2504\n",
      "Epoch 15/50\n",
      "12375/12375 [==============================] - 28s 2ms/step - loss: 2.1548 - acc: 0.3248 - val_loss: 2.4956 - val_acc: 0.2365\n",
      "Epoch 16/50\n",
      "12375/12375 [==============================] - 29s 2ms/step - loss: 2.1281 - acc: 0.3340 - val_loss: 2.5566 - val_acc: 0.2438\n",
      "Epoch 17/50\n",
      "12375/12375 [==============================] - 28s 2ms/step - loss: 2.0879 - acc: 0.3462 - val_loss: 2.5314 - val_acc: 0.2392\n",
      "Epoch 18/50\n",
      "12375/12375 [==============================] - 28s 2ms/step - loss: 2.0565 - acc: 0.3524 - val_loss: 2.5616 - val_acc: 0.2453\n",
      "Epoch 19/50\n",
      "12375/12375 [==============================] - 28s 2ms/step - loss: 2.0226 - acc: 0.3573 - val_loss: 2.6012 - val_acc: 0.2302\n",
      "Epoch 20/50\n",
      "12375/12375 [==============================] - 28s 2ms/step - loss: 1.9923 - acc: 0.3719 - val_loss: 2.5649 - val_acc: 0.2293\n",
      "Epoch 21/50\n",
      "12375/12375 [==============================] - 28s 2ms/step - loss: 1.9647 - acc: 0.3762 - val_loss: 2.6149 - val_acc: 0.2351\n",
      "Epoch 22/50\n",
      "12375/12375 [==============================] - 28s 2ms/step - loss: 1.9236 - acc: 0.3871 - val_loss: 2.6523 - val_acc: 0.2441\n",
      "Epoch 23/50\n",
      "12375/12375 [==============================] - 28s 2ms/step - loss: 1.9088 - acc: 0.3927 - val_loss: 2.6564 - val_acc: 0.2220\n",
      "Epoch 24/50\n",
      "12375/12375 [==============================] - 28s 2ms/step - loss: 1.8765 - acc: 0.3996 - val_loss: 2.7001 - val_acc: 0.2399\n",
      "Epoch 25/50\n",
      "12375/12375 [==============================] - 28s 2ms/step - loss: 1.8572 - acc: 0.4062 - val_loss: 2.7495 - val_acc: 0.2414\n",
      "Epoch 26/50\n",
      "12375/12375 [==============================] - 28s 2ms/step - loss: 1.8092 - acc: 0.4221 - val_loss: 2.7583 - val_acc: 0.2305\n",
      "Epoch 27/50\n",
      "12375/12375 [==============================] - 28s 2ms/step - loss: 1.7972 - acc: 0.4238 - val_loss: 2.7480 - val_acc: 0.2361\n",
      "Epoch 28/50\n",
      "12375/12375 [==============================] - 28s 2ms/step - loss: 1.7681 - acc: 0.4341 - val_loss: 2.8016 - val_acc: 0.2298\n",
      "Epoch 29/50\n",
      "12375/12375 [==============================] - 28s 2ms/step - loss: 1.7346 - acc: 0.4395 - val_loss: 2.7990 - val_acc: 0.2189\n",
      "Epoch 30/50\n",
      "12375/12375 [==============================] - 28s 2ms/step - loss: 1.7174 - acc: 0.4492 - val_loss: 2.8990 - val_acc: 0.2332\n",
      "Epoch 31/50\n",
      "12375/12375 [==============================] - 28s 2ms/step - loss: 1.6958 - acc: 0.4583 - val_loss: 2.8426 - val_acc: 0.2237\n",
      "Epoch 32/50\n",
      "12375/12375 [==============================] - 28s 2ms/step - loss: 1.6765 - acc: 0.4637 - val_loss: 3.0191 - val_acc: 0.2324\n",
      "Epoch 33/50\n",
      "12375/12375 [==============================] - 28s 2ms/step - loss: 1.6551 - acc: 0.4692 - val_loss: 2.8838 - val_acc: 0.2288\n",
      "Epoch 34/50\n",
      "12375/12375 [==============================] - 28s 2ms/step - loss: 1.6280 - acc: 0.4743 - val_loss: 2.9830 - val_acc: 0.2201\n",
      "Epoch 35/50\n",
      "12375/12375 [==============================] - 28s 2ms/step - loss: 1.6039 - acc: 0.4886 - val_loss: 2.9742 - val_acc: 0.2235\n",
      "Epoch 36/50\n",
      "12375/12375 [==============================] - 28s 2ms/step - loss: 1.5877 - acc: 0.4845 - val_loss: 3.0027 - val_acc: 0.2399\n",
      "Epoch 37/50\n",
      "12375/12375 [==============================] - 28s 2ms/step - loss: 1.5630 - acc: 0.4985 - val_loss: 3.0023 - val_acc: 0.2138\n",
      "Epoch 38/50\n",
      "12375/12375 [==============================] - 28s 2ms/step - loss: 1.5614 - acc: 0.4939 - val_loss: 3.0543 - val_acc: 0.2046\n",
      "Epoch 39/50\n",
      "12375/12375 [==============================] - 28s 2ms/step - loss: 1.5183 - acc: 0.5093 - val_loss: 3.0750 - val_acc: 0.2089\n",
      "Epoch 40/50\n",
      "12375/12375 [==============================] - 28s 2ms/step - loss: 1.5107 - acc: 0.5112 - val_loss: 3.1286 - val_acc: 0.2244\n",
      "Epoch 41/50\n",
      "12375/12375 [==============================] - 28s 2ms/step - loss: 1.5011 - acc: 0.5136 - val_loss: 3.1546 - val_acc: 0.2247\n",
      "Epoch 42/50\n",
      "12375/12375 [==============================] - 28s 2ms/step - loss: 1.4702 - acc: 0.5255 - val_loss: 3.2262 - val_acc: 0.2341\n",
      "Epoch 43/50\n",
      "12375/12375 [==============================] - 28s 2ms/step - loss: 1.4672 - acc: 0.5236 - val_loss: 3.2159 - val_acc: 0.2215\n",
      "Epoch 44/50\n",
      "12375/12375 [==============================] - 28s 2ms/step - loss: 1.4459 - acc: 0.5316 - val_loss: 3.2682 - val_acc: 0.2247\n",
      "Epoch 45/50\n",
      "12375/12375 [==============================] - 28s 2ms/step - loss: 1.4259 - acc: 0.5362 - val_loss: 3.2011 - val_acc: 0.2126\n",
      "Epoch 46/50\n",
      "12375/12375 [==============================] - 28s 2ms/step - loss: 1.4201 - acc: 0.5367 - val_loss: 3.2504 - val_acc: 0.2121\n",
      "Epoch 47/50\n",
      "12375/12375 [==============================] - 28s 2ms/step - loss: 1.3938 - acc: 0.5425 - val_loss: 3.3171 - val_acc: 0.2138\n",
      "Epoch 48/50\n",
      "12375/12375 [==============================] - 28s 2ms/step - loss: 1.3737 - acc: 0.5562 - val_loss: 3.4530 - val_acc: 0.2254\n",
      "Epoch 49/50\n",
      "12375/12375 [==============================] - 28s 2ms/step - loss: 1.3678 - acc: 0.5611 - val_loss: 3.2964 - val_acc: 0.2145\n",
      "Epoch 50/50\n",
      "12375/12375 [==============================] - 28s 2ms/step - loss: 1.3549 - acc: 0.5580 - val_loss: 3.3632 - val_acc: 0.2041\n",
      "4126/4126 [==============================] - 2s 448us/step\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=20, \n",
    "          epochs=50, validation_data=(x_valid, y_valid))\n",
    "\n",
    "\n",
    "score = model.evaluate(x_valid, y_valid, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score is [3.36317154301409, 0.20407174308786563]\n"
     ]
    }
   ],
   "source": [
    "print('accuracy score is', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14850 samples, validate on 1651 samples\n",
      "Epoch 1/10\n",
      "14850/14850 [==============================] - 32s 2ms/step - loss: 1.8390 - acc: 0.4648 - val_loss: 1.5307 - val_acc: 0.5936\n",
      "Epoch 2/10\n",
      "14850/14850 [==============================] - 32s 2ms/step - loss: 1.7841 - acc: 0.4684 - val_loss: 1.5663 - val_acc: 0.5669\n",
      "Epoch 3/10\n",
      "14850/14850 [==============================] - 32s 2ms/step - loss: 1.7099 - acc: 0.4798 - val_loss: 1.5870 - val_acc: 0.5512\n",
      "Epoch 4/10\n",
      "14850/14850 [==============================] - 32s 2ms/step - loss: 1.6916 - acc: 0.4780 - val_loss: 1.6279 - val_acc: 0.5391\n",
      "Epoch 5/10\n",
      "14850/14850 [==============================] - 32s 2ms/step - loss: 1.6626 - acc: 0.4851 - val_loss: 1.6434 - val_acc: 0.5142\n",
      "Epoch 6/10\n",
      "14850/14850 [==============================] - 32s 2ms/step - loss: 1.6433 - acc: 0.4896 - val_loss: 1.6748 - val_acc: 0.5051\n",
      "Epoch 7/10\n",
      "14850/14850 [==============================] - 32s 2ms/step - loss: 1.6080 - acc: 0.4957 - val_loss: 1.7254 - val_acc: 0.4870\n",
      "Epoch 8/10\n",
      "14850/14850 [==============================] - 32s 2ms/step - loss: 1.5999 - acc: 0.4989 - val_loss: 1.7613 - val_acc: 0.4797\n",
      "Epoch 9/10\n",
      "14850/14850 [==============================] - 32s 2ms/step - loss: 1.5912 - acc: 0.5028 - val_loss: 1.7896 - val_acc: 0.4682\n",
      "Epoch 10/10\n",
      "14850/14850 [==============================] - 32s 2ms/step - loss: 1.5581 - acc: 0.5106 - val_loss: 1.7868 - val_acc: 0.4688\n",
      "1651/1651 [==============================] - 1s 444us/step\n",
      "Score is :  [1.7868220156283179, 0.4688067847782738]\n",
      "Train on 14851 samples, validate on 1650 samples\n",
      "Epoch 1/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.6142 - acc: 0.4980 - val_loss: 1.1257 - val_acc: 0.6703\n",
      "Epoch 2/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.5646 - acc: 0.5102 - val_loss: 1.1784 - val_acc: 0.6461\n",
      "Epoch 3/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.5504 - acc: 0.5104 - val_loss: 1.2201 - val_acc: 0.6261\n",
      "Epoch 4/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.5313 - acc: 0.5140 - val_loss: 1.2936 - val_acc: 0.5921\n",
      "Epoch 5/10\n",
      "14851/14851 [==============================] - 33s 2ms/step - loss: 1.5037 - acc: 0.5233 - val_loss: 1.3120 - val_acc: 0.6006\n",
      "Epoch 6/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.4896 - acc: 0.5243 - val_loss: 1.3750 - val_acc: 0.5764\n",
      "Epoch 7/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.4618 - acc: 0.5330 - val_loss: 1.4048 - val_acc: 0.5697\n",
      "Epoch 8/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.4551 - acc: 0.5363 - val_loss: 1.4712 - val_acc: 0.5582\n",
      "Epoch 9/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.4285 - acc: 0.5443 - val_loss: 1.4935 - val_acc: 0.5376\n",
      "Epoch 10/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.4148 - acc: 0.5542 - val_loss: 1.5340 - val_acc: 0.5242\n",
      "1650/1650 [==============================] - 1s 437us/step\n",
      "Score is :  [1.5339739553856127, 0.5242424253261451]\n",
      "Train on 14851 samples, validate on 1650 samples\n",
      "Epoch 1/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.4788 - acc: 0.5315 - val_loss: 0.9250 - val_acc: 0.7479\n",
      "Epoch 2/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.4387 - acc: 0.5403 - val_loss: 0.9493 - val_acc: 0.7261\n",
      "Epoch 3/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.4339 - acc: 0.5415 - val_loss: 1.0293 - val_acc: 0.7012\n",
      "Epoch 4/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.3958 - acc: 0.5480 - val_loss: 1.0599 - val_acc: 0.6745\n",
      "Epoch 5/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.3944 - acc: 0.5555 - val_loss: 1.1013 - val_acc: 0.6545\n",
      "Epoch 6/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.3725 - acc: 0.5596 - val_loss: 1.1269 - val_acc: 0.6485\n",
      "Epoch 7/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.3791 - acc: 0.5604 - val_loss: 1.1565 - val_acc: 0.6339\n",
      "Epoch 8/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.3528 - acc: 0.5676 - val_loss: 1.2172 - val_acc: 0.6218\n",
      "Epoch 9/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.3357 - acc: 0.5705 - val_loss: 1.2154 - val_acc: 0.6170\n",
      "Epoch 10/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.3256 - acc: 0.5760 - val_loss: 1.2392 - val_acc: 0.6115\n",
      "1650/1650 [==============================] - 1s 445us/step\n",
      "Score is :  [1.2391804333889123, 0.6115151538993373]\n",
      "Train on 14851 samples, validate on 1650 samples\n",
      "Epoch 1/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.3619 - acc: 0.5684 - val_loss: 0.7318 - val_acc: 0.7976\n",
      "Epoch 2/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.3422 - acc: 0.5752 - val_loss: 0.8045 - val_acc: 0.7709\n",
      "Epoch 3/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.3319 - acc: 0.5755 - val_loss: 0.8473 - val_acc: 0.7364\n",
      "Epoch 4/10\n",
      "14851/14851 [==============================] - 31s 2ms/step - loss: 1.3237 - acc: 0.5797 - val_loss: 0.9107 - val_acc: 0.7255\n",
      "Epoch 5/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.3208 - acc: 0.5795 - val_loss: 0.9325 - val_acc: 0.7170\n",
      "Epoch 6/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.2893 - acc: 0.5827 - val_loss: 1.0222 - val_acc: 0.6800\n",
      "Epoch 7/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.2806 - acc: 0.5888 - val_loss: 1.0103 - val_acc: 0.6800\n",
      "Epoch 8/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.2625 - acc: 0.5940 - val_loss: 1.0702 - val_acc: 0.6582\n",
      "Epoch 9/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.2580 - acc: 0.5990 - val_loss: 1.0777 - val_acc: 0.6509\n",
      "Epoch 10/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.2479 - acc: 0.6001 - val_loss: 1.1059 - val_acc: 0.6515\n",
      "1650/1650 [==============================] - 1s 442us/step\n",
      "Score is :  [1.105879454540484, 0.6515151536825932]\n",
      "Train on 14851 samples, validate on 1650 samples\n",
      "Epoch 1/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.2959 - acc: 0.5851 - val_loss: 0.6264 - val_acc: 0.8103\n",
      "Epoch 2/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.2852 - acc: 0.5958 - val_loss: 0.7113 - val_acc: 0.7909\n",
      "Epoch 3/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.2510 - acc: 0.5972 - val_loss: 0.7412 - val_acc: 0.7776\n",
      "Epoch 4/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.2301 - acc: 0.6056 - val_loss: 0.7966 - val_acc: 0.7545\n",
      "Epoch 5/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.2320 - acc: 0.6065 - val_loss: 0.8240 - val_acc: 0.7461\n",
      "Epoch 6/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.2090 - acc: 0.6117 - val_loss: 0.8290 - val_acc: 0.7442\n",
      "Epoch 7/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.2040 - acc: 0.6134 - val_loss: 0.8660 - val_acc: 0.7303\n",
      "Epoch 8/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.1856 - acc: 0.6217 - val_loss: 0.9135 - val_acc: 0.7109\n",
      "Epoch 9/10\n",
      "14851/14851 [==============================] - 33s 2ms/step - loss: 1.2051 - acc: 0.6119 - val_loss: 0.9590 - val_acc: 0.6885\n",
      "Epoch 10/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.1826 - acc: 0.6165 - val_loss: 0.9675 - val_acc: 0.6921\n",
      "1650/1650 [==============================] - 1s 440us/step\n",
      "Score is :  [0.967485066616174, 0.6921212138551655]\n",
      "Train on 14851 samples, validate on 1650 samples\n",
      "Epoch 1/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.2231 - acc: 0.6112 - val_loss: 0.6023 - val_acc: 0.8370\n",
      "Epoch 2/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.1998 - acc: 0.6158 - val_loss: 0.5962 - val_acc: 0.8436\n",
      "Epoch 3/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.1845 - acc: 0.6194 - val_loss: 0.6588 - val_acc: 0.7976\n",
      "Epoch 4/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.1827 - acc: 0.6193 - val_loss: 0.6800 - val_acc: 0.8030\n",
      "Epoch 5/10\n",
      "14851/14851 [==============================] - 33s 2ms/step - loss: 1.1612 - acc: 0.6243 - val_loss: 0.7226 - val_acc: 0.7788\n",
      "Epoch 6/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.1672 - acc: 0.6206 - val_loss: 0.7715 - val_acc: 0.7721\n",
      "Epoch 7/10\n",
      "14851/14851 [==============================] - 33s 2ms/step - loss: 1.1475 - acc: 0.6314 - val_loss: 0.7972 - val_acc: 0.7430\n",
      "Epoch 8/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.1354 - acc: 0.6327 - val_loss: 0.8029 - val_acc: 0.7521\n",
      "Epoch 9/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.1202 - acc: 0.6371 - val_loss: 0.8327 - val_acc: 0.7364\n",
      "Epoch 10/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.1269 - acc: 0.6395 - val_loss: 0.8704 - val_acc: 0.7267\n",
      "1650/1650 [==============================] - 1s 440us/step\n",
      "Score is :  [0.8703722404711174, 0.7266666654384497]\n",
      "Train on 14851 samples, validate on 1650 samples\n",
      "Epoch 1/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.1772 - acc: 0.6250 - val_loss: 0.5305 - val_acc: 0.8685\n",
      "Epoch 2/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.1425 - acc: 0.6315 - val_loss: 0.5520 - val_acc: 0.8497\n",
      "Epoch 3/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.1302 - acc: 0.6357 - val_loss: 0.6031 - val_acc: 0.8321\n",
      "Epoch 4/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.1175 - acc: 0.6426 - val_loss: 0.6391 - val_acc: 0.8182\n",
      "Epoch 5/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.1102 - acc: 0.6426 - val_loss: 0.6287 - val_acc: 0.8164\n",
      "Epoch 6/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.1038 - acc: 0.6468 - val_loss: 0.6703 - val_acc: 0.8000\n",
      "Epoch 7/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.0967 - acc: 0.6461 - val_loss: 0.6995 - val_acc: 0.7970\n",
      "Epoch 8/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.0888 - acc: 0.6455 - val_loss: 0.7564 - val_acc: 0.7673\n",
      "Epoch 9/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.0813 - acc: 0.6464 - val_loss: 0.7778 - val_acc: 0.7515\n",
      "Epoch 10/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.0665 - acc: 0.6618 - val_loss: 0.7636 - val_acc: 0.7582\n",
      "1650/1650 [==============================] - 1s 504us/step\n",
      "Score is :  [0.7636260442661517, 0.7581818190487948]\n",
      "Train on 14851 samples, validate on 1650 samples\n",
      "Epoch 1/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.1205 - acc: 0.6414 - val_loss: 0.4602 - val_acc: 0.8727\n",
      "Epoch 2/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.0983 - acc: 0.6439 - val_loss: 0.4979 - val_acc: 0.8655\n",
      "Epoch 3/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.0738 - acc: 0.6498 - val_loss: 0.5255 - val_acc: 0.8600\n",
      "Epoch 4/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.0719 - acc: 0.6522 - val_loss: 0.5602 - val_acc: 0.8352\n",
      "Epoch 5/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.0716 - acc: 0.6539 - val_loss: 0.5757 - val_acc: 0.8352\n",
      "Epoch 6/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.0410 - acc: 0.6621 - val_loss: 0.6089 - val_acc: 0.8230\n",
      "Epoch 7/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.0425 - acc: 0.6628 - val_loss: 0.6332 - val_acc: 0.8012\n",
      "Epoch 8/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.0438 - acc: 0.6600 - val_loss: 0.6574 - val_acc: 0.8061\n",
      "Epoch 9/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.0388 - acc: 0.6659 - val_loss: 0.6740 - val_acc: 0.7952\n",
      "Epoch 10/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.0285 - acc: 0.6658 - val_loss: 0.6747 - val_acc: 0.7933\n",
      "1650/1650 [==============================] - 1s 443us/step\n",
      "Score is :  [0.6746731552210721, 0.7933333335500775]\n",
      "Train on 14851 samples, validate on 1650 samples\n",
      "Epoch 1/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.0828 - acc: 0.6534 - val_loss: 0.4475 - val_acc: 0.8891\n",
      "Epoch 2/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.0448 - acc: 0.6648 - val_loss: 0.4744 - val_acc: 0.8739\n",
      "Epoch 3/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.0549 - acc: 0.6621 - val_loss: 0.5169 - val_acc: 0.8558\n",
      "Epoch 4/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.0269 - acc: 0.6723 - val_loss: 0.5198 - val_acc: 0.8558\n",
      "Epoch 5/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.0259 - acc: 0.6707 - val_loss: 0.5415 - val_acc: 0.8473\n",
      "Epoch 6/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.0065 - acc: 0.6750 - val_loss: 0.5709 - val_acc: 0.8333\n",
      "Epoch 7/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.0024 - acc: 0.6771 - val_loss: 0.5854 - val_acc: 0.8230\n",
      "Epoch 8/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.0114 - acc: 0.6732 - val_loss: 0.6292 - val_acc: 0.8103\n",
      "Epoch 9/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 0.9998 - acc: 0.6739 - val_loss: 0.6490 - val_acc: 0.7994\n",
      "Epoch 10/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.0048 - acc: 0.6800 - val_loss: 0.6943 - val_acc: 0.7830\n",
      "1650/1650 [==============================] - 1s 473us/step\n",
      "Score is :  [0.6943450747114239, 0.7830303047642563]\n",
      "Train on 14851 samples, validate on 1650 samples\n",
      "Epoch 1/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.0275 - acc: 0.6692 - val_loss: 0.4419 - val_acc: 0.8739\n",
      "Epoch 2/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.0190 - acc: 0.6706 - val_loss: 0.4704 - val_acc: 0.8721\n",
      "Epoch 3/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.0110 - acc: 0.6721 - val_loss: 0.4932 - val_acc: 0.8642\n",
      "Epoch 4/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 0.9883 - acc: 0.6808 - val_loss: 0.4924 - val_acc: 0.8552\n",
      "Epoch 5/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 1.0007 - acc: 0.6735 - val_loss: 0.5361 - val_acc: 0.8448\n",
      "Epoch 6/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 0.9545 - acc: 0.6876 - val_loss: 0.5488 - val_acc: 0.8339\n",
      "Epoch 7/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 0.9710 - acc: 0.6884 - val_loss: 0.5953 - val_acc: 0.8242\n",
      "Epoch 8/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 0.9604 - acc: 0.6907 - val_loss: 0.6222 - val_acc: 0.8079\n",
      "Epoch 9/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 0.9557 - acc: 0.6919 - val_loss: 0.6365 - val_acc: 0.8048\n",
      "Epoch 10/10\n",
      "14851/14851 [==============================] - 32s 2ms/step - loss: 0.9517 - acc: 0.6941 - val_loss: 0.6650 - val_acc: 0.7873\n",
      "1650/1650 [==============================] - 1s 443us/step\n",
      "Score is :  [0.6650268899671959, 0.7872727285731923]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "for tridx, tsidx in kfold.split(padded_sentences, embeding_labels):\n",
    "    # \"Fit the model\" \n",
    "    model.fit(padded_sentences[tridx], embeding_labels[tridx], batch_size=20, \n",
    "              epochs=10, validation_data=(padded_sentences[tsidx], embeding_labels[tsidx]))\n",
    "\n",
    "    # Evaluate the trained model\n",
    "    score = model.evaluate(padded_sentences[tsidx], embeding_labels[tsidx], batch_size=20) \n",
    "    print('Score is : ', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
